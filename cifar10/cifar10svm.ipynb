{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog&Cat  SVM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team members:\n",
    "* Zhichun Wang ID 19024898\n",
    "* Ziyi Wang   ID 18042783\n",
    "* Youzhi Lei  ID 19039281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "* Using SVM to classify some kinds of vehicles & animals\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about datasets\n",
    "Labeled vehicles and animals from:\n",
    "    Images of various types of vehicles and animals http://www.cs.toronto.edu/%7Ekriz/cifar.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import patsy\n",
    "import os\n",
    "from PIL import Image \n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.2\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "    # resize the image to a fixed size, then flatten the image into\n",
    "    # a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()\n",
    "\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
    "    # handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "    # if imutils.is_cv2():\n",
    "    #    hist = cv2.normalize(hist)\n",
    "    #print(\"line2\")\n",
    "    # otherwise, perform \"in place\" normalization in OpenCV 3\n",
    "    #else:\n",
    "    cv2.normalize(hist, hist)\n",
    "    \n",
    "    # return the flattened histogram as the feature vector\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "#sns.set_style(\"whitegrid\")\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "rcParams['figure.figsize'] = 10,8\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['axes.labelsize'] = 'large'\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dp1/Documents/fred/codes/machinelearning2019/cifar10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "#Please put the data to current location, Thanks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dp1/Documents/fred/codes/machinelearning2019/cifar10/../cifar-10-batches-py\n"
     ]
    }
   ],
   "source": [
    "#load the dataset \n",
    "#test one image and show \n",
    "DATADIR = \"../cifar-10-batches-py\"\n",
    "DATADIR = os.path.join(cwd, DATADIR)\n",
    "print(DATADIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is a b'frog' . Its label is 6\n",
      "10 categories: [b'airplane', b'automobile', b'bird', b'cat', b'deer', b'dog', b'frog', b'horse', b'ship', b'truck']\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Choose one image randomly, just to display it\n",
    "batch = unpickle(DATADIR+\"/data_batch_1\")\n",
    "\n",
    "# For more detail on the dataset files, see http://www.cs.toronto.edu/%7Ekriz/cifar.html \n",
    "IMG_SIZE = 32 # each image in the dataset is 32*32, color\n",
    "img_array = batch[b'data'][0].reshape(3, 1024)\n",
    "ziplist = list(zip(img_array[2], img_array[1], img_array[0]))\n",
    "newarray = np.array(ziplist).reshape(IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "#plt.imshow(newarray)\n",
    "#plt.show\n",
    "\n",
    "img_array = cv2.cvtColor(newarray, cv2.COLOR_BGR2GRAY)\n",
    "#plt.imshow(img_array, cmap='gray')\n",
    "#plt.show\n",
    "\n",
    "metafile = unpickle(DATADIR+\"/batches.meta\") \n",
    "print(\"This image is a\", metafile[b'label_names'][batch[b'labels'][0]], \". Its label is\", batch[b'labels'][0])\n",
    "\n",
    "CATEGORIES = []\n",
    "for i in range(len(metafile[b'label_names'])):\n",
    "    CATEGORIES.append(metafile[b'label_names'][i])\n",
    "\n",
    "print(len(metafile[b'label_names']), \"categories:\", CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 images are in data_batch_5\n",
      "10000 images are in data_batch_2\n",
      "10000 images are in data_batch_3\n",
      "10000 images are in data_batch_4\n",
      "10000 images are in test_batch\n",
      "10000 images are in data_batch_1\n"
     ]
    }
   ],
   "source": [
    "rawImages = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "def create_training_data():    \n",
    "    for batchfile in os.listdir(DATADIR):        \n",
    "        if (batchfile.startswith(\"data_batch_\") or batchfile == 'test_batch'):                \n",
    "            batch = unpickle(os.path.join(DATADIR, batchfile))\n",
    "            print(len(batch[b'data']), \"images are in\", batchfile)\n",
    "            for i in range(len(batch[b'data'])): # There should be 10000 images in each batch                    \n",
    "                # 1024 red values, then 1024 green values, then 1024 blue values\n",
    "                img_array = batch[b'data'][i].reshape(3, 1024) \n",
    "                #print(\"shape is\", img_array.shape)\n",
    "                # Note that OpenCV needs BGR instead of RGB\n",
    "                ziplist = list(zip(img_array[2], img_array[1], img_array[0])) \n",
    "                img_array = np.array(ziplist).reshape(IMG_SIZE, IMG_SIZE, 3)\n",
    "                pixels = image_to_feature_vector(img_array)    \n",
    "                hist = extract_color_histogram(img_array)       \n",
    "                label = batch[b'labels'][i]\n",
    "                \n",
    "                rawImages.append(pixels)\n",
    "                features.append(hist)\n",
    "                labels.append(label)                    \n",
    "create_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(rawImages))\n",
    "print(len(features))\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pixels matrix: 180.00MB\n",
      "[INFO] features matrix: 120.00MB\n",
      "[1 8 5 ... 1 1 5]\n"
     ]
    }
   ],
   "source": [
    "# show some information on the memory consumed by the raw images\n",
    "# matrix and features matrix\n",
    "rawImages = np.array(rawImages)\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] pixels matrix: {:.2f}MB\".format(rawImages.nbytes / (1024 * 1000.0)))\n",
    "print(\"[INFO] features matrix: {:.2f}MB\".format(features.nbytes / (1024 * 1000.0)))\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#features_scaled = preprocessing.scale(features)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_scaled  = scaler.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits, using 85%\n",
    "# of the data for training and the remaining 15% for testing\n",
    "(trainRI, testRI, trainRL, testRL) = train_test_split(rawImages, labels, test_size=0.15, random_state=42)\n",
    "(trainFeat, testFeat, trainLabels, testLabels) = train_test_split(features, labels, test_size=0.15, random_state=42)\n",
    "#(trainFeat, testFeat, trainLabels, testLabels) = train_test_split(features_scaled, labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(trainRI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] evaluating raw pixel accuracy...\n"
     ]
    }
   ],
   "source": [
    "# k-NN\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(trainRI, trainRL)\n",
    "acc = model.score(testRI, testRL)\n",
    "#print(\"[INFO] k-NN classifier: k=%d\" % args[\"neighbors\"])\n",
    "print(\"[INFO] raw pixel accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "model = MLPClassifier(hidden_layer_sizes=(32,), max_iter=1000, alpha=1e-4,\n",
    "                      solver='sgd', tol=1e-4, random_state=1,\n",
    "                      learning_rate_init=.1)\n",
    "model.fit(trainRI, trainRL)\n",
    "acc = model.score(testRI, testRL)\n",
    "print(\"[INFO] neural network raw pixel accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating histogram accuracy...\")\n",
    "model = MLPClassifier(hidden_layer_sizes=(32,), max_iter=1000, alpha=1e-4,\n",
    "                      solver='sgd', tol=1e-4, random_state=1,\n",
    "                      learning_rate_init=.1)\n",
    "model.fit(trainFeat, trainLabels)\n",
    "acc = model.score(testFeat, testLabels)\n",
    "print(\"[INFO] neural network histogram accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] evaluating raw pixel accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SVM-SVC raw pixel accuracy: 10.64%\n",
      "CPU times: user 48min 59s, sys: 699 ms, total: 49min\n",
      "Wall time: 48min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#SVC\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "model = SVC(max_iter=1000,class_weight='balanced')\n",
    "model.fit(trainRI, trainRL)\n",
    "acc = model.score(testRI, testRL)\n",
    "print(\"[INFO] SVM-SVC raw pixel accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] evaluating histogram accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SVM-SVC histogram accuracy: 10.43333333%\n",
      "CPU times: user 7min 38s, sys: 116 ms, total: 7min 38s\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#SVC\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating histogram accuracy...\")\n",
    "model = SVC(max_iter=1000,class_weight='balanced')\n",
    "model.fit(trainFeat, trainLabels)\n",
    "acc = model.score(testFeat, testLabels)\n",
    "print(\"[INFO] SVM-SVC histogram accuracy: {:.8f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
