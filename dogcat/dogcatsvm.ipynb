{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog&Cat  SVM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team members:\n",
    "* Ziyi Wang   ID 18042783\n",
    "* Youzhi Lei  ID 19039281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "* Using SVM to classify the dog&cat\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about datasets\n",
    "#### Labeled dogs and cats from Microsoft\n",
    "* Dogs and Cats image https://www.microsoft.com/en-us/download/details.aspx?id=54765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import patsy\n",
    "import os\n",
    "from PIL import Image \n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.2\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(50, 50)):\n",
    "    # resize the image to a fixed size, then flatten the image into\n",
    "    # a list of raw pixel intensities\n",
    "    return cv2.resize(image, size).flatten()\n",
    "\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
    "    # handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "    # if imutils.is_cv2():\n",
    "    #    hist = cv2.normalize(hist)\n",
    "    #print(\"line2\")\n",
    "    # otherwise, perform \"in place\" normalization in OpenCV 3\n",
    "    #else:\n",
    "    cv2.normalize(hist, hist)\n",
    "    \n",
    "    # return the flattened histogram as the feature vector\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "#sns.set_style(\"whitegrid\")\n",
    "rcParams['figure.dpi'] = 150\n",
    "rcParams['lines.linewidth'] = 2\n",
    "rcParams['axes.facecolor'] = 'white'\n",
    "rcParams['patch.edgecolor'] = 'white'\n",
    "rcParams['font.family'] = 'StixGeneral'\n",
    "rcParams['figure.figsize'] = 10,8\n",
    "rcParams['font.size'] = 14\n",
    "rcParams['axes.labelsize'] = 'large'\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dp1/Documents/fred/codes/machinelearning2019/dogcat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "#Please put the data to current location, Thanks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dp1/Documents/fred/codes/machinelearning2019/dogcat/../kagglecatsanddogs/PetImages\n"
     ]
    }
   ],
   "source": [
    "#load the dataset \n",
    "#test one image and show \n",
    "DATADIR = \"../kagglecatsanddogs/PetImages\"\n",
    "DATADIR = os.path.join(cwd, DATADIR)\n",
    "print(DATADIR)\n",
    "CATEGORIES = [\"Dog\",\"Cat\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawImages = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        #conver the dog and cat to numerical value 0/1\n",
    "        label = CATEGORIES.index(category)\n",
    "        \n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                #read  \n",
    "                imgpath = os.path.join(path,img)\n",
    "                \n",
    "                img_array = cv2.imread(imgpath)\n",
    "\n",
    "                pixels = image_to_feature_vector(img_array)\n",
    "                \n",
    "                hist = extract_color_histogram(img_array)\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(imgpath)\n",
    "                os.remove(imgpath) \n",
    "                continue\n",
    "                \n",
    "            rawImages.append(pixels)\n",
    "            features.append(hist)\n",
    "            labels.append(label)\n",
    "\n",
    "            \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n",
      "24946\n",
      "24946\n"
     ]
    }
   ],
   "source": [
    "print(len(rawImages))\n",
    "print(len(features))\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pixels matrix: 182.71MB\n",
      "[INFO] features matrix: 49.89MB\n"
     ]
    }
   ],
   "source": [
    "# show some information on the memory consumed by the raw images\n",
    "# matrix and features matrix\n",
    "rawImages = np.array(rawImages)\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] pixels matrix: {:.2f}MB\".format(rawImages.nbytes / (1024 * 1000.0)))\n",
    "print(\"[INFO] features matrix: {:.2f}MB\".format(features.nbytes / (1024 * 1000.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d49fffc2474b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#features_scaled = preprocessing.scale(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfeatures_scaled\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#features_scaled = preprocessing.scale(features)\n",
    "scaler = StandardScaler()\n",
    "features_scaled  = scaler.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits, using 85%\n",
    "# of the data for training and the remaining 15% for testing\n",
    "(trainRI, testRI, trainRL, testRL) = train_test_split(rawImages, labels, test_size=0.15, random_state=42)\n",
    "(trainFeat, testFeat, trainLabels, testLabels) = train_test_split(features, labels, test_size=0.15, random_state=42)\n",
    "#(trainFeat, testFeat, trainLabels, testLabels) = train_test_split(features_scaled, labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset = {\n",
    "    'trainRI': trainRI,\n",
    "    'testRI' : testRI,\n",
    "    'trainRL': trainRL,\n",
    "    'testRL' : testRL\n",
    "}\n",
    "filename = 'rawImagesdataset.pickle'\n",
    "\n",
    "outfile = open(filename,'wb+')\n",
    "print(outfile)\n",
    "pickle.dump(dataset,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dataset = {\n",
    "    'trainFeat': trainFeat,\n",
    "    'testFeat' : testFeat,\n",
    "    'trainLabels': trainLabels,\n",
    "    'testLabels' : testLabels\n",
    "}\n",
    "filename = 'features_scaleddataset.pickle'\n",
    "\n",
    "outfile = open(filename,'wb+')\n",
    "print(outfile)\n",
    "pickle.dump(dataset,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21204, 7500)\n",
      "(3742, 7500)\n",
      "(21204,)\n",
      "(3742,)\n",
      "(21204, 512)\n",
      "(3742, 512)\n",
      "(21204,)\n",
      "(3742,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[7.7037728e-01 8.6041335e-03 1.5518170e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [4.9476840e-02 1.5552505e-02 1.6915949e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [1.2886506e-02 1.0173556e-02 2.5128685e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [6.3553471e-03 3.2998919e-03 2.1999279e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [6.5048292e-02 1.4669396e-02 2.7075514e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [4.9178828e-03 2.0058766e-02 1.1740961e-01 ... 1.5489395e-04\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "[1 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(trainRI.shape)\n",
    "print(testRI.shape)\n",
    "print(trainRL.shape)\n",
    "print(testRL.shape)\n",
    "\n",
    "print(trainFeat.shape)\n",
    "print(testFeat.shape)\n",
    "print(trainLabels.shape)\n",
    "print(testLabels.shape)\n",
    "\n",
    "print(type(trainRI))\n",
    "print(type(testRI))\n",
    "print(type(trainRL))\n",
    "print(type(testRL))\n",
    "\n",
    "print(type(trainFeat))\n",
    "print(type(testFeat))\n",
    "print(type(trainLabels))\n",
    "print(type(testLabels))\n",
    "\n",
    "print(testFeat)\n",
    "print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeat = np.array(testFeat)\n",
    "testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[9.0343070e-01 2.4944368e-01 6.7670859e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 4.0895315e-03 7.9453751e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [8.2122593e-04 5.1619918e-03 1.8418925e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 4.2775902e-04 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [1.1264091e-02 1.3338332e-01 5.0276309e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [2.7554853e-02 4.9082082e-02 2.2388319e-02 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(testFeat))\n",
    "print(type(testLabels))\n",
    "\n",
    "print(testFeat)\n",
    "print(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] evaluating raw pixel accuracy...\n",
      "[INFO] raw pixel accuracy: 55.05%\n"
     ]
    }
   ],
   "source": [
    "# k-NN\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(trainRI, trainRL)\n",
    "acc = model.score(testRI, testRL)\n",
    "#print(\"[INFO] k-NN classifier: k=%d\" % args[\"neighbors\"])\n",
    "print(\"[INFO] raw pixel accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, alpha=1e-4,\n",
    "                      solver='sgd', tol=1e-4, random_state=1,\n",
    "                      learning_rate_init=.1)\n",
    "model.fit(trainRI, trainRL)\n",
    "acc = model.score(testRI, testRL)\n",
    "print(\"[INFO] neural network raw pixel accuracy: {:.2f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating histogram accuracy...\")\n",
    "model = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, alpha=1e-4,\n",
    "                      solver='sgd', tol=1e-4, random_state=1,\n",
    "                      learning_rate_init=.1)\n",
    "model.fit(trainFeat, trainLabels)\n",
    "acc = model.score(testFeat, testLabels)\n",
    "print(\"[INFO] neural network histogram accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rawImagesdataset.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    rawImagesdataset = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "(trainRI, testRI, trainRL, testRL) =   rawImagesdataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] evaluating raw pixel accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "model = SVC(max_iter=1000,class_weight='balanced')\n",
    "model.fit(trainRI, trainRL)\n",
    "acc = model.score(testRI, testRL)\n",
    "print(\"[INFO] SVM-SVC raw pixel accuracy: {:.2f}%\".format(acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('features_scaleddataset.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    features_scaleddataset = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "(trainRI, testRI, trainRL, testRL) =   features_scaleddataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] evaluating histogram accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:244: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SVM-SVC histogram accuracy: 51.01549973%\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "print(\"\\n\")\n",
    "print(\"[INFO] evaluating histogram accuracy...\")\n",
    "model = SVC(max_iter=1000,class_weight='balanced')\n",
    "model.fit(trainFeat, trainLabels)\n",
    "acc = model.score(testFeat, testLabels)\n",
    "print(\"[INFO] SVM-SVC histogram accuracy: {:.8f}%\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
